#+STARTUP: overview

I wonder: can we normalize \(p(t,a) / Z\) i.e. find the probability of \(t\)? I get the feeling we're just missing Binomial coefficients.

* Data structures
** Trie
- benchmark
  - 10000 uniform queries took 35s
  - shortest 10% account for 10s while longest 10% accounts for only 1.3s
  - times decay super-linearly from shortest to longest queries
- profile run on initial implementation reveals 95% of the running time consists of support queries
- 63% spent in select function of edge implementation
  - 60% spent on hash maps
    - 15.5% iterating over hash map
    - 25% get_inner (mystery?)
    - 20% contains_key
  - hashing accounts for 8.4%
- calculating is only 0.5%
- fitting is not measurable
- is_partial_superset requires 25%
  - 20% spent comparing path item < query item
* Bernoulli model
** Transaction distribution
We define a generative model for i.i.d. transactions \(t\).
Let there be a pattern set \(P \subseteq 2^\mathcal{I}\) for a set of items \(\mathcal{I}\).
Let there be indicators for the presence of a pattern \(a_{t,p}\) for \(p \in P\).
We model transactions as the result of a random draw of the indicator vectors
\[ a_{tp} \sim Bern(\theta_p) \]
\[ z^+_{ti} \sim Cat(\alpha_i, \beta_i, \gamma_i) \]  
\[ z^-_{ti} \sim Bern(\theta^-_i) \]
where the latter two indicate additive and destructive noise respectively.
The transaction is obtained as the union of all patterns with destructive noise removed and additive noise added.
\[ t = \bigcup_{p \in P: a_{tp} = 1} p \setminus \{ i \in \mathcal{I} \mid z^-_{ti} = - \} \cup \{ i \in \mathcal{I} \mid z^+_{ti} = + \} \]
Thus, for a given transaction \(t\) a given choice for the assignment vector constrains the choices for the noise vector.
\[ i \in t \land i \not \in cover(a_t) \implies z_{ti} = + \]
\[ i \not \in t \land i \in cover(a_t) \implies z_{ti} = - \]
We have the joint distribution
\[ p(t, a_t) = p(t | a_t)p(a_t) \]
By independence of the patterns
\[ p(a_t) = \prod_{p \in P} p(a_{tp}) \]
To obtain the probability of a transaction, we need to consider all possible assignments of noise that could have generated \(t\).
\[ p(t \mid a_t) = \sum_{z_t} p(z_t \mid a_t) \left[ t = \bigcup_{p \in P: a_{tp} = 1} p \setminus \{i \in \mathcal{I} \mid z_{ti} = -\} \cup \{i \in \mathcal{I} \mid z^+_{ti} = +\} \right] \]
We observe that the indicators for different items are independent given \(a_t\) and \(t_i\), so we can factor the function
\[ p(t \mid a_t) = \prod_{i} \sum_{z_{ti}} p(z_{ti}, t_i \mid a_t) \]
From the constraints above we derive
\[ p(z_{ti}, t_i \mid a_t) = \left\{
   \begin{matrix}
     \alpha_i & \text{ if } z_{ti} = + \land t_i = 1 \\
     \beta_i & \text{ if } z_{ti} = - \land t_i = 0 \\
     \gamma_i & \text{ if } z_{ti} = 0 \land t_i = 1 \iff i \in cover( a_t ) \\
     0 & \text{ otherwise } \\
     \end{matrix} \]
Note that all cases are disjoint. 
Thus the sum over the noise assignments is
\[ \sum_{z_{ti}} p(z_{ti}, t_i \mid a_t) = p(t_i \mid a_t) \]
For a fixed transaction \(t\) and assignment \(a_t\) consider the covering \(v = \bigcup_{p \in P: a_{tp} = 1}  p\).
Rewrite
\[ p(t_i \mid a_t) = t_i \alpha_i  + (1 - t_i) \beta_i + [t_i = v_i] \gamma_i \]
Rewrite \(\gamma_i = 1 - \alpha_i - \beta_i\) and splitting the cases
\[\gamma_i^{[t_i = v_i]} = t_i v_i (1 - \alpha_i - \beta_i) + (1 - t_i) (1 - v_i)(1 - \alpha_i - \beta_i) \]
Substitute and expand \(t_i = t_i v_i + t_i (1 - v_i)\) and \(1 - t_i = (1 - t_i) v_i + (1 - t_i) (1 - v_i)\)
\[p(t_i \mid a_t) = t_i v_i \alpha_i + t_i(1 - v_i) \alpha_i + (1 - t_i) v_i \beta_i + (1 - t_i)(1 - v_i) \beta_i + t_i v_i (1 - \alpha_i - \beta_i) + (1 - t_i)(1 - v_i)(1 - \alpha_i - \beta_i)\]
Collect terms and rearrange
\[p(t_i \mid a_t) = v_i (t_i (1 - \beta_i) + (1 - t_i) \beta_i) + (1 - v_i) (t_i \alpha_i + (1 - t_i)(1 - \alpha_i))\]
Observe that the probability is Bernoulli given \(v_i\).
Rewrite as a product
\[p(t_i \mid a_t) = p(t_i \mid v_i) = \alpha_i^{(1 - v_i)t_i} (1 - \alpha_i)^{(1 - v_i)(1 - t_i)} \beta_i^{v_i(1 - t_i)} (1 - \beta_i)^{v_i t_i}\]
Thus, we cover \(t_i = 1\) by additive noise depending on whether it is covered with Bernoulli probability \(\alpha_i\).
Likewise, we remove \(v_i = 1\) by destructive noise depending on whether it is present in \(t\) with Bernoulli probability \(\beta_i\). 
** Data log likelihood
We analyze the log likelihood of a set of \(n\) i.i.d. transactions \(T\) under a given assignment \(A\).
\[\log p(T, A) = \sum_{t \in T} \log p(t, a_t)\]
Substituting the model
\[\sum_t \log p(t, a_t) = \sum_t \log p(t \mid a_t) + \log p(a_t)\]
First, we simplify the pattern sum
\begin{align*}
  \sum_t \log p(a_t)
    & = \sum_t \sum_p \log p(a_{tp}) \\
    & = \sum_p \sum_t a_{tp} \log \theta_p + (1 - a_{tp}) \log(1 - \theta_p)
\end{align*}
For a fixed assignment \(A\) we rewrite using the counts \(c(p)\)
\[ \sum_t \log p(a_t) = \sum_p c(p) \log \theta_p + (n - c(p)) \log(1 - \theta_p) \]
Next, simplify the sum over the transactions conditioned on the assignments with \(V\) being the cover of \(T\) under \(A\)
\[ \sum_t \log p(t \mid a_t) = \sum_t \sum_i \log p(t_i \mid v_{ti})\]
More compactly
\[ \log p(T | A) = \sum_i \log p(T|_i \mid V|_i = 0) + \log p(T|_i \mid V|_i = 1) \]
We rewrite using the count statistics \(c(i, t_i, v_i)\).
\[ \sum_t \log p(t \mid a_t) = \sum_i c(i, 1, 0) \log \alpha_i + c(i, 0, 0) \log(1 - \alpha_i) + c(i, 0, 1) \log \beta_i + c(i, 1, 1) \log(1 - \beta_i) \]
** Maximum likelihood fit
Let T be a set of \(n\) i.i.d. transactions.
*** Pattern Parameters
The pattern parameters \(\theta_p\) do not interact among themselves nor with the noise parameters, so we can maximize them independently
\[\max_{\theta_p} \log p(T, A) = \max_{\theta_p} \sum_t \log(a_{tp} ; \theta_p )\]
This is the MLE of a Bernoulli parameter, which is the empirical probability
\[\hat{\theta}_p = \frac{c(p)}{n}\]
*** Additive noise parameters
The additive noise parameter \(\alpha_i\) is also independent of all other parameters for a given assignment matrix A.
Further, the parameter only occurs if \(v_i = 0\).
\[\hat{\alpha}_i = argmax_{\alpha_i} \sum_t p(t_i \mid v_{ti} = 0 ; \alpha_i)\]
Thus the MLE is again the empirical probability over all transactions where \(v_{ti} = 0\)
\[ \hat{\alpha}_i = \frac{c(i, 1, 0)}{c(i, 1, 0) + c(i, 0, 0)} \] 
*** Destructive noise parameters
Analogously to the additive noise parameter we obtain the MLE
\[ \hat{\beta}_i = argmax_{\beta_i} \sum_t p(t_i \mid v_{ti} = 1 ; \beta_i) = \frac{c(i, 0, 1)}{c(i, 0, 1) + c(i, 1, 1)} \] 
** Gain estimation
We want to know the effect of adding a pattern to the log likelihood.
We define
\[\Delta(p, P) = \max_{A' \in A(P \cup \{p\}), \Theta'} p(T, A' ; \Theta') - \max_{A \in A(P), \Theta} p(T, A ; \Theta) \]
*** Crude gain estimation
In the general case, it is difficult to obtain the assignments and associated count statistics to calculate the log likelihood.
It is very simple, however, if there are no patterns in the model yet.
In this case, we can attribute the change in noise probability directly to the pattern we added.
*Assumption 1* The gain over the empty model is proportional to the gain in the actual model.
The gain is
\[ \Delta(p) = \max_{A, \Theta'} p(T, A ; \Theta') - \max_{\Theta} p(T ; \Theta) \]
In the empty model, we would use a pattern to cover all transactions that support.
We can use it on even more transactions, but calculating the effect while considering destructive noise is costly.
*Assumption 2* The maximum gain when adding to the empty model is obtained by a transaction with a pattern if and only if the pattern is supported.
Since we consider the improvement over the empty model, this fixes all assignments.
As the assignments \(A\) are fixed, the parameters are fixed too by their MLE.
Thus, we can calculate the gain
\begin{align*}
  \Delta(p) & = p(T, A; \hat{\Theta}') - p(T ; \hat{\Theta}) \\
  & = \sum_t \log p(a_{tp} ; \hat{\theta}_p) + \sum_i \log p(t_i \mid a_{tp} ; \hat{\alpha}'_i, \hat{\beta}'_i ) - \log p(t_i ; \hat{\alpha}_i, \hat{\beta}_i)
\end{align*}
We calculate the pattern probability using its support giving the count \(c(p) = sup(p)\) and MLE
\[\sum_t p(a_{tp} ; \hat{\theta_p}) = sup(p) \log \frac{sup(p)}{n} + (n - sup(p)) \log \frac{sup(n - p)}{n}\]
Since the empty model needs to explain every item as additive noise, we reduce the noise counts by \(sup(p)\) by using the new pattern \(p\) to explain every transaction that supports it.
Thus, the count of the additive noise item \(i\) in the new model is \(sup(i) - sup(p)\).
Further, neither the empty model nor the model augmented by \(p\) uses any destructive noise.
The MLE \(\hat{\beta}'_i = \hat{\beta}_i = 0\) implies that all terms for destructive noise vanish.
The MLE for additive noise parameters are \(\hat{\alpha}'_i = \frac{sup(i) - sup(p)}{n - sup(p)}\) and \(\hat{\alpha}_i = \frac{sup(i)}{n - sup(i)}\)
We plug the estimates into the liklihoods
\[ \sum_t \sum_i \log p(t_i \mid a_{tp} ; \hat{\alpha}'_i, \hat{\beta}'_i) = \sum_i (sup(i) - sup(p)) \log \frac{sup(i) - sup(p)}{n - sup(p)} + (n - sup(i)) \log \frac{n - sup(i)}{n - sup(p)} \]
\[ \sum_t \sum_i \log p(t_i ; \hat{\alpha}_i, \hat{\beta}_i) = \sum_i sup(i) \log \frac{sup(i)}{n} + (n - sup(i)) \log \frac{n - sup(i)}{n} \]
Observe that the noise count is reduced by \(sup(p)\).
Thus, any gain in likelihood comes from removing occurrences of items that need to be explained by noise.
** Maximum a posteriori fit
*** Posterior distribution
MLE is problematic, espcecially for gain estimation, because it may produce gains such as \(\log 0 - \log 0\).
We use a prior distribution to avoid this case by spreading some probability everywhere.
Thus, we have to treat the parameter set \(\Theta\) as a set of random variables
\[p(A, \Theta \mid T) \propto p(A, T \mid \Theta) p(\Theta) \]
The likelihood is known \(p(A, T \mid \Theta) = p(A, T ; \Theta)\).
We impose independent priors over every model parameter.
\[p(\Theta) = \prod_{p \in P} p(\theta_p) \prod_{i \in \mathcal{I}} p(\alpha_i, \beta_i, \gamma_i)\]
Since the Beta distribution is conjugate of the Bernoulli distribution we impose \(\alpha_i \sim Beta(a_+,a_-)\), \(\beta_i \sim Beta(b_+, b_-)\) and \(\theta_p \sim Beta(c_+, c_-)\)
The liklihood and priors are local as
\[p(A, T \mid \Theta) p(\Theta) = \prod_{p \in P} p(\theta_p) p(a_p) \prod_{i \in \mathcal{I}}  p(T|_i \mid V|_i = 0, \alpha_i) p(\alpha_i) p(T|_i \mid V|_i = 1, \beta_i) p(\beta_i)\]
The product of a Bernoulli and a Beta distribution is
\[p(\theta_p) p(a_p) = \frac{1}{B(c_+, c_-)} \theta_p^{c_p + c_+ - 1} (1 - \theta_p)^{n - c_p + c_- - 1}\]
Similarly
\[ p(T|_i \mid V|_i = 0, \alpha_i) p(\alpha_i) = \frac{1}{B(a_+, a_-)} \alpha_i^{c(i,1,0) + a_+ - 1} (1 - \alpha_i)^{c(i,0,0) + a_- - 1} \]
\[ p(T|_i \mid V|_i = 1, \beta_i) p(\beta_i) = \frac{1}{B(b_+, b_-)} \beta_i^{c(i,0,1) + b_+ - 1} (1 - \beta_i)^{c(i,1,1) + b_- - 1} \]
*** Maximum a posteriori estimates
The maximum a posteriori estimate is a biased version of the maximum likelihood estimate

\[ \hat{\theta}_p} = \frac{c_p + c_+ - 1}{n + c_+ + c_- - 2} \]
\[ \hat{\alpha_i} = \frac{c(i,1,0) + a_+ - 1}{c(i,1,0) + c(i,0,0) + a_+ + a_- - 2} \]
\[ \hat{\beta_i} = \frac{c(i,0,1) + b_+ - 1}{c(i,0,1) + c(i,1,1) + b_+ + b_- - 2} \] 
* Improvements
** Data structures
- cache (short) queries as trie
- use skip graph to navigate the edge list faster
- provide batched query to reduce traversal
- parallelize traversal
** Model
- integrate out parameters
- sum out assignments
  - Gibb's sampler to estimate the pmf?
** Search
- Stochastic search MCTS
- Maintain residual trie that contains uncompressed additive noise (search = traversal?)
